{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:30:37.033005Z",
     "start_time": "2025-01-11T15:30:37.010120Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from feat import Detector\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from PIL import Image\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:30:37.129506Z",
     "start_time": "2025-01-11T15:30:37.056913Z"
    }
   },
   "source": [
    "file_path = './DiffusionFER/DiffusionEmotion_S/dataset_sheet.csv'  # Replace with the actual path to your dataset file\n",
    "\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "dataset['subDirectory_filePath'] = dataset['subDirectory_filePath'].apply(lambda x: x.split('/')[-1])\n",
    "dataset['subDirectory_filePath'] = dataset['subDirectory_filePath'].str.replace('.png', '', regex=False)\n",
    "\n",
    "# Display the first few rows to verify structure\n",
    "print(dataset.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subDirectory_filePath  valence  arousal  expression\n",
      "0            aksjlkjl_0     -0.1      0.1           0\n",
      "1            aksndlkn_0      0.0      0.0           0\n",
      "2            anavqmjd_0     -0.1     -0.1           0\n",
      "3            aovjrrax_0     -0.2     -0.1           0\n",
      "4            aptzlpuo_0     -0.1     -0.1           0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:30:51.755825Z",
     "start_time": "2025-01-11T15:30:37.208686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = \"./DiffusionFER/DiffusionEmotion_S/cropped/\"\n",
    "\n",
    "i = 1\n",
    "image_paths = []\n",
    "labels = []\n",
    "image_files = []\n",
    "for folder_name in os.listdir(data_path):\n",
    "    i += 1\n",
    "    if i == 6:\n",
    "        break\n",
    "    image_folder = data_path  + folder_name\n",
    "    images = os.listdir(image_folder)\n",
    "    for image in images:\n",
    "        labels.append(folder_name)\n",
    "        image_paths.append(image_folder + f'/{image}')\n",
    "        image_files.append(cv2.imread(image_paths[-1]))\n",
    "\n",
    "print(len(image_files), len(labels))\n",
    "\n",
    "\n",
    "detector = Detector(device=\"cpu\")\n",
    "\n",
    "\n",
    "# List to store AU data for CSV\n",
    "aus_data = []\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {'image_paths': image_paths,\n",
    "     'labels': labels,\n",
    "    })\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Face model: retinaface\n",
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\face_detectors\\Retinaface\\Retinaface_test.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(\n",
      "INFO:root:Loading Facial Landmark model: mobilefacenet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678 678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\detector.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "INFO:root:Loading facepose model: img2pose\n",
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_test.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=self.device)\n",
      "INFO:root:Loading AU model: xgb\n",
      "INFO:root:Loading emotion model: resmasknet\n",
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\emo_detectors\\ResMaskNet\\resmasknet_test.py:718: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate the accuracy of the model\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:30:52.094236Z",
     "start_time": "2025-01-11T15:30:51.973869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "def train_and_eval(model, train_in, train_out, val_in, val_out):\n",
    "\n",
    "    model.fit(train_in, train_out)\n",
    "    predicted_val = model.predict(val_in)\n",
    "    with open('model.pkl','wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "\n",
    "    # Evaluate model\n",
    "    return accuracy_score(val_out, predicted_val)\n",
    "\n",
    "\n",
    "def train_and_eval_regressor(model, train_in, train_out, val_in, val_out):\n",
    "    model.fit(train_in, train_out)\n",
    "    predicted_val = model.predict(val_in)\n",
    "    # Evaluate model using regression metrics\n",
    "    mse = mean_squared_error(val_out, predicted_val)\n",
    "    r2 = r2_score(val_out, predicted_val)\n",
    "    return mse, r2"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:30:54.446021Z",
     "start_time": "2025-01-11T15:30:52.214556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "au_file_path = './aus.csv'  # Replace with the actual path to your dataset file\n",
    "\n",
    "au = pd.read_csv(au_file_path)\n",
    "au['file'] = au['file'].apply(lambda x: x.split('/')[-1])\n",
    "au.rename(columns={'file': 'subDirectory_filePath'}, inplace=True)\n",
    "\n",
    "# merge the dataframe with the the au and emotion values\n",
    "dataset = pd.merge(dataset, au, on='subDirectory_filePath', how='inner')\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Find rows with any NaN values\n",
    "rows_with_nan = dataset[dataset.isnull().any(axis=1)]\n",
    "print(rows_with_nan)\n",
    "\n",
    "# Drop multiple columns\n",
    "features = dataset.drop(columns=['valence', 'arousal', 'expression', 'face', 'subDirectory_filePath'])\n",
    "\n",
    "# Extract the target column\n",
    "labels = dataset['expression']\n",
    "\n",
    "data_in, test_in, data_out, test_out = train_test_split(\n",
    "features,\n",
    "labels,\n",
    "test_size=0.1,\n",
    "random_state=42,\n",
    "stratify=labels # balances labels across the sets\n",
    ")\n",
    "train_in, val_in, train_out, val_out = train_test_split(\n",
    "data_in,\n",
    "data_out,\n",
    "test_size=(0.2/0.9), # 20% of the original data\n",
    "random_state=42,\n",
    "stratify=data_out\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subDirectory_filePath    0\n",
      "valence                  0\n",
      "arousal                  0\n",
      "expression               0\n",
      "face                     0\n",
      "AU01                     6\n",
      "AU02                     6\n",
      "AU04                     6\n",
      "AU05                     6\n",
      "AU06                     6\n",
      "AU07                     6\n",
      "AU09                     6\n",
      "AU10                     6\n",
      "AU11                     6\n",
      "AU12                     6\n",
      "AU14                     6\n",
      "AU15                     6\n",
      "AU17                     6\n",
      "AU20                     6\n",
      "AU23                     6\n",
      "AU24                     6\n",
      "AU25                     6\n",
      "AU26                     6\n",
      "AU28                     6\n",
      "AU43                     6\n",
      "dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [subDirectory_filePath, valence, arousal, expression, face, AU01, AU02, AU04, AU05, AU06, AU07, AU09, AU10, AU11, AU12, AU14, AU15, AU17, AU20, AU23, AU24, AU25, AU26, AU28, AU43]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train with Logistic Regression"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:30:56.120159Z",
     "start_time": "2025-01-11T15:30:54.735834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_accuracy = train_and_eval(logistic_model, train_in, train_out, val_in, val_out)\n",
    "print(\"Logistic Regression Validation Accuracy:\", logistic_accuracy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Logistic Regression Validation Accuracy: 0.6703703703703704\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and emotion detection with trained model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:30:56.372247Z",
     "start_time": "2025-01-11T15:30:56.353311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import cv2\n",
    "# import opencv_jupyter_ui as jcv2\n",
    "# from feat import Detector\n",
    "# from IPython.display import Image\n",
    "#\n",
    "# from feat.utils import FEAT_EMOTION_COLUMNS\n",
    "#\n",
    "# cam = cv2.VideoCapture(0)\n",
    "# cam.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "#\n",
    "# while True:\n",
    "#     ret, frame = cam.read()\n",
    "#     if not ret:\n",
    "#         print(\"OpenCV found an error reading the next frame.\")\n",
    "#         break\n",
    "#\n",
    "#     faces = detector.detect_faces(frame)\n",
    "#     landmarks = detector.detect_landmarks(frame, faces)\n",
    "#     emotions = detector.detect_emotions(frame, faces, landmarks)\n",
    "#\n",
    "#     # The functions seem to assume a collection of images or frames. We acces \"frame 0\".\n",
    "#     faces = faces[0]\n",
    "#     landmarks = landmarks[0]\n",
    "#     emotions = emotions[0]\n",
    "#\n",
    "#     strongest_emotion = emotions.argmax(axis=1)\n",
    "#\n",
    "#     for (face, top_emo) in zip(faces, strongest_emotion):\n",
    "#         (x0, y0, x1, y1, p) = face\n",
    "#         cv2.rectangle(frame, (int(x0), int(y0)), (int(x1), int(y1)), (255, 0, 0), 3)\n",
    "#         cv2.putText(frame, FEAT_EMOTION_COLUMNS[top_emo], (int(x0), int(y0 - 10)), cv2.FONT_HERSHEY_PLAIN, 1.5, (255, 0, 0), 2)\n",
    "#\n",
    "#     jcv2.imshow(\"Emotion Detection\", frame)\n",
    "#\n",
    "#     key = jcv2.waitKey(1) & 0xFF\n",
    "#     if key == 27: # ESC pressed\n",
    "#         break\n",
    "#\n",
    "# cam.release()\n",
    "# jcv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:40:07.145546Z",
     "start_time": "2025-01-11T15:38:12.695520Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "from feat import Detector\n",
    "\n",
    "# Initialize Py-Feat detector\n",
    "detector = Detector()\n",
    "\n",
    "\n",
    "\n",
    "def capture_and_process_image(cap, face_tracker, detector):\n",
    "    \"\"\"\n",
    "    Captures an image from the webcam, saves it temporarily, and processes it using the detector.\n",
    "    \"\"\"\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        return None\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_tracker.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    if len(faces) == 0:\n",
    "        print(\"No face detected.\")\n",
    "        return None\n",
    "\n",
    "    # Process the first detected face (modify if you want to process multiple faces)\n",
    "    x, y, w, h = faces[0]\n",
    "    face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "    # Save the face ROI as a temporary image\n",
    "    temp_image_path = \"temp_face.jpg\"\n",
    "    cv2.imwrite(temp_image_path, face_roi)\n",
    "\n",
    "    try:\n",
    "        # Pass the image path to the detector\n",
    "        result = detector.detect_image(temp_image_path)\n",
    "        # Predict the emotion using the Random Forest model\n",
    "        face_features = result.aus.values.flatten()\n",
    "        with open('model.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        emotion = model.predict([face_features])[0]  # Model expects 2D array\n",
    "        print(emotion)\n",
    "            # Display the predicted emotion\n",
    "        cv2.putText(frame, f'Emotion: {emotion}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Delete the temporary image\n",
    "        if os.path.exists(temp_image_path):\n",
    "            os.remove(temp_image_path)\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during detection: {e}\")\n",
    "        # Clean up the temporary file in case of error\n",
    "        if os.path.exists(temp_image_path):\n",
    "            os.remove(temp_image_path)\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    face_tracker = cv2.CascadeClassifier(\"frontal_face_features.xml\")\n",
    "\n",
    "    # Open the laptop webcam (default device is 0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the webcam is opened correctly\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not access the webcam.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"Press 'Enter' to capture and process an image.\")\n",
    "    print(\"Press 'ESC' to exit.\")\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If the frame was not captured correctly, break the loop\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        # Display the live feed\n",
    "        cv2.imshow(\"Live Feed\", frame)\n",
    "\n",
    "        # Check for key presses\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:  # Press 'ESC' to exit the program\n",
    "            break\n",
    "        elif key == 13:  # Press 'Enter' to capture and process an image\n",
    "            detection_result = capture_and_process_image(cap, face_tracker, detector)\n",
    "\n",
    "\n",
    "\n",
    "    # Release the capture and close the OpenCV window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Face model: retinaface\n",
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\face_detectors\\Retinaface\\Retinaface_test.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(\n",
      "INFO:root:Loading Facial Landmark model: mobilefacenet\n",
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\detector.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "INFO:root:Loading facepose model: img2pose\n",
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_test.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=self.device)\n",
      "INFO:root:Loading AU model: xgb\n",
      "INFO:root:Loading emotion model: resmasknet\n",
      "C:\\Users\\Alina Lånedator\\iis\\Lib\\site-packages\\feat\\emo_detectors\\ResMaskNet\\resmasknet_test.py:718: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'Enter' to capture and process an image.\n",
      "Press 'ESC' to exit.\n",
      "No face detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]INFO:root:detecting faces...\n",
      "INFO:root:detecting landmarks...\n",
      "INFO:root:RESCALING WARNING: image_operations.extract_face_from_bbox() is rescaling cropped img with shape torch.Size([3, 267, 267]) to 112\n",
      "INFO:root:detecting poses...\n",
      "INFO:root:img2pose: RESCALING WARNING: img2pose has a min img size of 400 and a max img size of 1400 but checked value is torch.Size([267, 267]).\n",
      "INFO:root:detecting aus...\n",
      "INFO:root:detecting emotions...\n",
      "INFO:root:inverting face transform...\n",
      "INFO:root:inverting landmark transform...\n",
      "INFO:root:creating fex output...\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:40:07.396254Z",
     "start_time": "2025-01-11T15:40:07.387680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Function to load and preprocess images\n",
    "# def load_and_preprocess_images(image_paths, image_size=(64, 64)):\n",
    "#     \"\"\"\n",
    "#     Loads and preprocesses images from the provided paths.\n",
    "#     Resizes images to the given image_size and flattens them into 1D arrays.\n",
    "#     \"\"\"\n",
    "#     images = []\n",
    "#     for image_path in image_paths:\n",
    "#         # Open the image file\n",
    "#         img = Image.open(image_path)\n",
    "#         # Resize image to a fixed size\n",
    "#         img = img.resize(image_size)\n",
    "#         # Convert image to numpy array and flatten it\n",
    "#         img_array = np.array(img).flatten()\n",
    "#         images.append(img_array)\n",
    "#\n",
    "#     return np.array(images)\n",
    "#\n",
    "# # Assuming 'dataset' is your DataFrame containing image paths and labels\n",
    "# # Example dataset structure\n",
    "# data = pd.DataFrame({\n",
    "#     'image_paths': data[\"image_paths\"],  # List of image paths\n",
    "#     'labels': data[\"labels\"]  # Corresponding labels\n",
    "# })\n",
    "# for image_path in data['image_paths']:\n",
    "#\n",
    "#     result = detector.detect_image(image_path)\n",
    "#     num_rows = len(result)\n",
    "#     new_df = result.aus\n",
    "#     image_path = image_path.replace(\".png\", \"\")\n",
    "#     image_path = image_path.replace(\"./DiffusionFER/DiffusionEmotion_S/cropped/\", \"\")\n",
    "#     new_df.insert(0, \"file\", image_path)\n",
    "#     new_df.insert(1, \"face\", range(num_rows))\n",
    "#\n",
    "#     aus_data.append(new_df)\n",
    "#\n",
    "#     aus_data.append(new_df)\n",
    "#\n",
    "#\n",
    "# # Merge all AU data and save as a CSV\n",
    "# merged_df = pd.concat(aus_data, ignore_index=True)\n",
    "# aus_df = pd.DataFrame(merged_df)\n",
    "# aus_df.to_csv(\"aus.csv\", index=False)\n",
    "# # Load and preprocess images\n",
    "# images = load_and_preprocess_images(data['image_paths'].values)\n",
    "#\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(images, data['labels'], test_size=0.2, random_state=42, stratify=data['labels'])\n",
    "#\n",
    "# # Train Random Forest model\n",
    "# rf_model = RandomForestClassifier(n_estimators=150, random_state=42, class_weight='balanced')\n",
    "# rf_model.fit(X_train, y_train)\n",
    "#\n",
    "# # Evaluate the model\n",
    "# y_pred = rf_model.predict(X_test)\n",
    "#\n",
    "# # Print evaluation results\n",
    "# # print(\"Accuracy on Test Set:\", accuracy_score(y_test, y_pred))\n",
    "# # print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# # print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T15:31:18.620501Z",
     "start_time": "2025-01-11T15:31:18.614733Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
